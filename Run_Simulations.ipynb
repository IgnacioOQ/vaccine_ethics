{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XMjcb55jT5jn",
        "1TvZ5E6aT7pf",
        "GR1CF8XU54IZ"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "XMjcb55jT5jn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUAFVU5S4DX",
        "outputId": "fd687485-ce9f-4ce0-8f3a-46cce16218d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vaccine_ethics'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 66 (delta 36), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (66/66), 23.66 KiB | 3.38 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "/content/vaccine_ethics\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IgnacioOQ/vaccine_ethics\n",
        "%cd vaccine_ethics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imports import *\n",
        "from agent_class import FullAgent\n",
        "from simulation_class import Simulation"
      ],
      "metadata": {
        "id": "s0ETUwUZTj-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "1TvZ5E6aT7pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_results = pd.DataFrame(columns=[\"step\", \"dead_proportion\", \"max_infected\", \"auc_infected\", \"avg_viral_age\", \"avg_immunity\"])\n",
        "\n",
        "simulation = Simulation(grid_size=25, num_agents=600, agent_class = FullAgent, init_infected_proportion = 0.5,\n",
        "                 proportion_vulnerable=0.1, vul_penalty = 0.5,\n",
        "                 infection_prob=0.25, recovery_time=30, death_prob=0.05,\n",
        "                 vax_vulnerable=False,\n",
        "                 vax_all=False,\n",
        "                 vax_effect = 0.7,\n",
        "                 viral_age_effect = 0.1,\n",
        "                 immune_adaptation_effect = 0.1,\n",
        "                 plot=False)\n",
        "simulation.run(500)  # Run for 20 iterations\n",
        "# simulation.plot_hist()\n",
        "results = simulation.generate_simulation_report()\n",
        "simulation_results.loc[len(simulation_results)] = results\n",
        "# simulation_results = pd.concat([simulation_results, results], ignore_index=True)\n",
        "simulation_results"
      ],
      "metadata": {
        "id": "TTXFo3V8Uc4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Bayesian Optimization with Parallelization"
      ],
      "metadata": {
        "id": "GR1CF8XU54IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwD_c8Ea6Cvz",
        "outputId": "47f18e33-3d9f-41ff-c693-f76fe1ce34dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "from skopt import Optimizer\n",
        "from skopt.space import Real\n",
        "from skopt.plots import plot_convergence\n",
        "\n",
        "# --- Simulation Constants (Scaled Up) ---\n",
        "grid_size = 100\n",
        "num_agents = 5000\n",
        "init_infected_proportion = 0.1\n",
        "proportion_vulnerable = 0.1\n",
        "infection_prob = 0.25\n",
        "recovery_time = 30\n",
        "\n",
        "# --- Search Space ---\n",
        "space = [\n",
        "    Real(0.01, 0.1, name='death_prob'),\n",
        "    Real(0.2, 0.8, name='vul_penalty'),\n",
        "    Real(0.5, 0.9, name='vax_effect'),\n",
        "    Real(0.05, 0.2, name='viral_age_effect'),\n",
        "    Real(0.05, 0.2, name='immune_adaptation_effect')\n",
        "]\n",
        "\n",
        "# --- Objective Function ---\n",
        "\n",
        "def run_simulation_for_params(params):\n",
        "    \"\"\"Run simulation for both vax_all True/False & return deaths difference.\"\"\"\n",
        "    death_prob, vul_penalty, vax_effect, viral_age_effect, immune_adaptation_effect = params\n",
        "    deaths = {}\n",
        "\n",
        "    for vax_all in [True, False]:\n",
        "        simulation = Simulation(\n",
        "            grid_size=grid_size,\n",
        "            num_agents=num_agents,\n",
        "            agent_class=FullAgent,\n",
        "            init_infected_proportion=init_infected_proportion,\n",
        "            proportion_vulnerable=proportion_vulnerable,\n",
        "            vul_penalty=vul_penalty,\n",
        "            infection_prob=infection_prob,\n",
        "            recovery_time=recovery_time,\n",
        "            death_prob=death_prob,\n",
        "            vax_vulnerable=True,\n",
        "            vax_all=vax_all,\n",
        "            vax_effect=vax_effect,\n",
        "            viral_age_effect=viral_age_effect,\n",
        "            immune_adaptation_effect=immune_adaptation_effect,\n",
        "            plot=False\n",
        "        )\n",
        "        simulation.run()\n",
        "        report = simulation.generate_simulation_report()\n",
        "        deaths[vax_all] = report[0]  # Assuming report[0] = dead count\n",
        "\n",
        "    difference = deaths[True] - deaths[False]\n",
        "    return difference\n",
        "\n",
        "# --- Parallel Objective Function ---\n",
        "\n",
        "def parallel_objective(params_batch, progress_desc=\"Running Batch\", num_workers=None):\n",
        "    \"\"\"Parallel version with dynamic CPU worker usage.\"\"\"\n",
        "    if num_workers is None:\n",
        "        num_workers = cpu_count()\n",
        "    results = Parallel(n_jobs=num_workers)(\n",
        "        delayed(run_simulation_for_params)(params) for params in tqdm(params_batch, desc=progress_desc)\n",
        "    )\n",
        "    return results\n",
        "\n",
        "# --- Extract Near-Optimal Region ---\n",
        "\n",
        "def get_best_region(df_results, threshold_percent=10):\n",
        "    \"\"\"Filter parameter sets within threshold % of the best result.\"\"\"\n",
        "    best_val = df_results['deaths_diff (vax_all True - False)'].min()\n",
        "    threshold = abs(best_val) * (threshold_percent / 100)\n",
        "    near_best = df_results[\n",
        "        abs(df_results['deaths_diff (vax_all True - False)'] - best_val) <= threshold\n",
        "    ]\n",
        "    print(f\"\\nFound {len(near_best)} parameter sets within {threshold_percent}% of the best result.\")\n",
        "    near_best.to_csv('near_best_parameter_region.csv', index=False)\n",
        "    print(\"\\nSaved near-optimal parameter sets to 'near_best_parameter_region.csv'.\")\n",
        "    print(near_best[['death_prob', 'vul_penalty', 'vax_effect', 'viral_age_effect', 'immune_adaptation_effect']])\n",
        "    return near_best\n",
        "\n",
        "# --- Bayesian Optimization Pipeline ---\n",
        "\n",
        "def run_bayesian_optimization(\n",
        "    n_calls=500,\n",
        "    n_initial_points=50,\n",
        "    parallel_batch_size=50,\n",
        "    threshold_percent=10,\n",
        "    num_workers=None\n",
        "):\n",
        "    print(\"\\n--- Starting Bayesian Optimization ---\")\n",
        "    results_list = []\n",
        "\n",
        "    def batch_objective(params_batch, desc):\n",
        "        batch_results = parallel_objective(params_batch, progress_desc=desc, num_workers=num_workers)\n",
        "        # Record results\n",
        "        for p, res in zip(params_batch, batch_results):\n",
        "            results_list.append({\n",
        "                'death_prob': p[0],\n",
        "                'vul_penalty': p[1],\n",
        "                'vax_effect': p[2],\n",
        "                'viral_age_effect': p[3],\n",
        "                'immune_adaptation_effect': p[4],\n",
        "                'deaths_diff (vax_all True - False)': res\n",
        "            })\n",
        "        return batch_results\n",
        "\n",
        "    # Initialize optimizer\n",
        "    opt = Optimizer(dimensions=space, base_estimator=\"GP\", acq_func=\"EI\", random_state=42)\n",
        "\n",
        "    # --- Initial Random Points ---\n",
        "    print(\"\\nGenerating initial random samples...\")\n",
        "    initial_params = opt.ask(n_initial_points)\n",
        "    batch_objective(initial_params, desc=\"Initial Random Sampling\")\n",
        "    opt.tell(initial_params, [r['deaths_diff (vax_all True - False)'] for r in results_list])\n",
        "\n",
        "    # --- Bayesian Iterations ---\n",
        "    num_batches = (n_calls - n_initial_points) // parallel_batch_size\n",
        "    for i in tqdm(range(num_batches), desc=\"Bayesian Optimization Progress\"):\n",
        "        next_params = opt.ask(parallel_batch_size)\n",
        "        batch_results = batch_objective(next_params, desc=f\"Batch {i+1}/{num_batches}\")\n",
        "        opt.tell(next_params, batch_results)\n",
        "\n",
        "    # --- Save Results ---\n",
        "    df_results = pd.DataFrame(results_list)\n",
        "    df_results.to_csv('bayesian_optimization_results.csv', index=False)\n",
        "    print(\"\\nBayesian Optimization complete! Results saved to 'bayesian_optimization_results.csv'.\")\n",
        "\n",
        "    # --- Best Params ---\n",
        "    best_idx = df_results['deaths_diff (vax_all True - False)'].idxmin()\n",
        "    print(\"\\nBest Parameters Found:\")\n",
        "    print(df_results.loc[best_idx])\n",
        "\n",
        "    # --- Convergence Plot ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plot_convergence(opt.get_result())\n",
        "    plt.show()\n",
        "\n",
        "    # --- Extract Near-Optimal Region ---\n",
        "    near_best_df = get_best_region(df_results, threshold_percent=threshold_percent)\n",
        "\n",
        "    return df_results, near_best_df\n",
        "\n",
        "# --- Run Everything ---\n",
        "df_results, near_best_df = run_bayesian_optimization(\n",
        "    n_calls=500,\n",
        "    n_initial_points=50,\n",
        "    parallel_batch_size=50,\n",
        "    threshold_percent=10,\n",
        "    num_workers=cpu_count()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "b7Yo5gXQ8aDx",
        "outputId": "5b109482-c0b2-4e4d-ad60-6f2fd8e0e312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Bayesian Optimization ---\n",
            "\n",
            "Generating initial random samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initial Random Sampling:  32%|███▏      | 16/50 [04:29<11:13, 19.80s/it]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ebe9e0d7f172>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# --- Run Everything ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m df_results, near_best_df = run_bayesian_optimization(\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mn_initial_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ebe9e0d7f172>\u001b[0m in \u001b[0;36mrun_bayesian_optimization\u001b[0;34m(n_calls, n_initial_points, parallel_batch_size, threshold_percent, num_workers)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGenerating initial random samples...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0minitial_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_initial_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mbatch_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Initial Random Sampling\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deaths_diff (vax_all True - False)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ebe9e0d7f172>\u001b[0m in \u001b[0;36mbatch_objective\u001b[0;34m(params_batch, desc)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_desc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Record results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ebe9e0d7f172>\u001b[0m in \u001b[0;36mparallel_objective\u001b[0;34m(params_batch, progress_desc, num_workers)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     results = Parallel(n_jobs=num_workers)(\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_simulation_for_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Gaussian Processes & Expected Improvement in Bayesian Optimization\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 What is a Gaussian Process (GP)?\n",
        "\n",
        "A **Gaussian Process (GP)** is a **probabilistic model** used to estimate unknown functions.\n",
        "\n",
        "### 🚀 Intuition:\n",
        "\n",
        "- Instead of fitting a single deterministic function, a GP models a **distribution over possible functions** that fit the observed data.\n",
        "- It assumes:\n",
        "  - **Any finite set of points** in the input space has a **joint multivariate Gaussian distribution**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 Key Components:\n",
        "\n",
        "1. **Mean function:**\n",
        "   - Denoted as:  \n",
        "     $$ m(x) = \\mathbb{E}[f(x)] $$\n",
        "   - Represents the expected value of the function at each point.  \n",
        "   - Often assumed to be zero for simplicity.\n",
        "\n",
        "2. **Covariance function (Kernel):**\n",
        "   - Denoted as:  \n",
        "     $$ k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))] $$\n",
        "   - Measures **similarity** between points.\n",
        "   - Common kernels:\n",
        "     - Radial Basis Function (RBF) / Squared Exponential.\n",
        "     - Matern kernel.\n",
        "   - Controls:\n",
        "     - Smoothness.\n",
        "     - Distance-based correlation.\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 How GPs Are Used in Optimization:\n",
        "\n",
        "Given some observed data points:\n",
        "\n",
        "1. **GP regression** predicts:\n",
        "   - **Mean prediction:**  \n",
        "     $$ \\mu(x) $$\n",
        "   - **Variance prediction (uncertainty):**  \n",
        "     $$ \\sigma^2(x) $$\n",
        "\n",
        "2. The optimizer uses both **$\\mu(x)$** and **$\\sigma(x)$** to decide where to evaluate next.\n",
        "\n",
        "---\n",
        "\n",
        "# 🟢 Expected Improvement (EI)\n",
        "\n",
        "## 🚀 What is Expected Improvement?\n",
        "\n",
        "- **EI is an acquisition function** used to balance:\n",
        "  - **Exploitation** → sample where the predicted objective value is best.\n",
        "  - **Exploration** → sample where the model is uncertain.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 EI Formula:\n",
        "\n",
        "Define:\n",
        "\n",
        "- **$f^*$**: Current best observed value (e.g., lowest deaths difference so far).\n",
        "- **$\\mu(x)$**: Predicted mean at point $x$ (from GP).\n",
        "- **$\\sigma(x)$**: Predicted standard deviation at point $x$.\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "EI(x) = \\mathbb{E}[\\max(0, f^* - f(x))]\n",
        "$$\n",
        "\n",
        "Closed-form formula:\n",
        "\n",
        "$$\n",
        "EI(x) = (f^* - \\mu(x)) \\cdot \\Phi(Z) + \\sigma(x) \\cdot \\phi(Z)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $$ Z = \\frac{f^* - \\mu(x)}{\\sigma(x)} $$\n",
        "- **$\\Phi(Z)$**: Cumulative distribution function (CDF) of standard normal.\n",
        "- **$\\phi(Z)$**: Probability density function (PDF) of standard normal.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 Intuition:\n",
        "\n",
        "- **If $\\mu(x)$ is much better than $f^*$** → EI is large → **Exploitation**.\n",
        "- **If $\\sigma(x)$ is large (high uncertainty)** → EI is large → **Exploration**.\n",
        "\n",
        "Thus, **EI naturally balances exploration and exploitation!**\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Summary:\n",
        "\n",
        "| Concept              | Meaning |\n",
        "|---------------------|--------|\n",
        "| **Gaussian Process** | Probabilistic model predicting both mean & uncertainty of objective function. |\n",
        "| **Mean $\\mu(x)$**    | Best guess of objective value at $x$. |\n",
        "| **Variance $\\sigma^2(x)$** | Uncertainty of the model at $x$. |\n",
        "| **Expected Improvement (EI)** | Acquisition function deciding next sample by maximizing improvement expectation. |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔥 Why It Works:\n",
        "\n",
        "- **GPs provide a smooth, uncertainty-aware model.**\n",
        "- **EI tells the optimizer where it's most beneficial to sample next:**\n",
        "  - Either improve the best value.\n",
        "  - Or reduce uncertainty in unknown areas.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Bonus:\n",
        "\n",
        "Would you like to see a visualization of how a Gaussian Process and Expected Improvement evolve over iterations on a toy function?\n"
      ],
      "metadata": {
        "id": "Ds2Gtc5T4wMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Parameter Search (parallelizable)"
      ],
      "metadata": {
        "id": "szb-eMAOhfEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool, cpu_count\n",
        "print(cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYTkbdDNyeSb",
        "outputId": "f103f2ec-aa1f-4ecd-fd46-936a286699a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fixed Simulation Settings ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "grid_size = 25\n",
        "num_agents = 600\n",
        "init_infected_proportion = 0.1\n",
        "proportion_vulnerable = 0.1\n",
        "infection_prob = 0.25\n",
        "recovery_time = 30\n",
        "\n",
        "base_sampling_ranges = {\n",
        "    'Death Prob': (0.01, 0.1),\n",
        "    'Vul Penalty': (0.2, 0.8),\n",
        "    'Vax Effect': (0.5, 0.9),\n",
        "    'Viral Age Effect': (0.05, 0.2),\n",
        "    'Immune Adaptation Effect': (0.05, 0.2)\n",
        "}\n",
        "\n",
        "# --- Parameter Sampling ---\n",
        "def sample_params(ranges):\n",
        "    return {param: round(random.uniform(*rng), 3) for param, rng in ranges.items()}\n",
        "\n",
        "# --- Worker Function: Single Simulation Task ---\n",
        "\n",
        "def run_single_simulation(params):\n",
        "    deaths = {}\n",
        "    for vax_all in [True, False]:\n",
        "        simulation = Simulation(\n",
        "            grid_size=grid_size,\n",
        "            num_agents=num_agents,\n",
        "            agent_class=FullAgent,\n",
        "            init_infected_proportion=init_infected_proportion,\n",
        "            proportion_vulnerable=proportion_vulnerable,\n",
        "            vul_penalty=params['Vul Penalty'],\n",
        "            infection_prob=infection_prob,\n",
        "            recovery_time=recovery_time,\n",
        "            death_prob=params['Death Prob'],\n",
        "            vax_vulnerable=True,\n",
        "            vax_all=vax_all,\n",
        "            vax_effect=params['Vax Effect'],\n",
        "            viral_age_effect=params['Viral Age Effect'],\n",
        "            immune_adaptation_effect=params['Immune Adaptation Effect'],\n",
        "            plot=False\n",
        "        )\n",
        "        simulation.run(500)\n",
        "        report = simulation.generate_simulation_report()\n",
        "        deaths[vax_all] = report[0]\n",
        "\n",
        "    # Check success condition\n",
        "    if deaths[False] < deaths[True]:\n",
        "        result = params.copy()\n",
        "        result.update({\n",
        "            'Deaths vax_all=False': deaths[False],\n",
        "            'Deaths vax_all=True': deaths[True]\n",
        "        })\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# --- Parallel Search Function ---\n",
        "\n",
        "def parallel_parameter_search(sampling_ranges, num_samples=500, stage_name='Stage', save_csv=False):\n",
        "    print(f\"\\n{stage_name}: Running {num_samples} samples in parallel...\")\n",
        "\n",
        "    # Prepare parameters to sample\n",
        "    param_list = [sample_params(sampling_ranges) for _ in range(num_samples)]\n",
        "\n",
        "    # Use all available CPUs\n",
        "    num_workers = cpu_count()\n",
        "    print(f\"Using {num_workers} CPU cores for parallelization.\")\n",
        "\n",
        "    with Pool(processes=num_workers) as pool:\n",
        "        # Parallel map with progress bar\n",
        "        results = list(tqdm.tqdm(pool.imap(run_single_simulation, param_list), total=num_samples))\n",
        "\n",
        "    # Filter out None results\n",
        "    successful_regions = [res for res in results if res is not None]\n",
        "\n",
        "    df_success = pd.DataFrame(successful_regions)\n",
        "    print(f\"{stage_name}: Found {len(df_success)} successful regions.\")\n",
        "\n",
        "    if save_csv:\n",
        "        df_success.to_csv(f'{stage_name.lower()}_successful_regions.csv', index=False)\n",
        "\n",
        "    return df_success\n",
        "\n",
        "# --- Analysis + Range Refinement ---\n",
        "\n",
        "def analyze_and_refine_ranges(df_success, shrink_factor=1.0):\n",
        "    refined_ranges = {}\n",
        "    params = list(base_sampling_ranges.keys())\n",
        "    stats = df_success[params].describe()\n",
        "\n",
        "    print(\"\\nRefining parameter ranges based on successful regions...\")\n",
        "    for param in params:\n",
        "        mean = stats.loc['mean', param]\n",
        "        std = stats.loc['std', param]\n",
        "        param_min = max(stats.loc['min', param], mean - shrink_factor * std)\n",
        "        param_max = min(stats.loc['max', param], mean + shrink_factor * std)\n",
        "        refined_ranges[param] = (round(param_min, 3), round(param_max, 3))\n",
        "\n",
        "        # Optional plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.histplot(df_success[param], bins=10, kde=True)\n",
        "        plt.title(f'{param} Distribution (Successful Regions)')\n",
        "        plt.show()\n",
        "\n",
        "    print(\"New refined parameter ranges:\", refined_ranges)\n",
        "    return refined_ranges\n",
        "\n",
        "# --- Recursive Refinement Pipeline ---\n",
        "\n",
        "def recursive_parallel_search(initial_samples=1000, focused_samples=500, num_iterations=3, shrink_factor=1.0):\n",
        "    print(f\"\\n--- Starting Recursive Parameter Search ({num_iterations} Iterations) ---\\n\")\n",
        "\n",
        "    # Step 1: Initial Random Search\n",
        "    df_initial = parallel_parameter_search(base_sampling_ranges, num_samples=initial_samples, stage_name='Initial', save_csv=True)\n",
        "    if df_initial.empty:\n",
        "        print(\"No successful regions found in initial search.\")\n",
        "        return\n",
        "\n",
        "    current_ranges = analyze_and_refine_ranges(df_initial, shrink_factor=shrink_factor)\n",
        "\n",
        "    # Step 2: Recursive Refinement\n",
        "    all_results = [df_initial]\n",
        "    for iteration in range(1, num_iterations + 1):\n",
        "        print(f\"\\n--- Iteration {iteration} ---\\n\")\n",
        "        df_refined = parallel_parameter_search(current_ranges, num_samples=focused_samples, stage_name=f'Focused_Iter{iteration}', save_csv=True)\n",
        "        if df_refined.empty:\n",
        "            print(f\"No successful regions found in iteration {iteration}. Stopping early.\")\n",
        "            break\n",
        "        all_results.append(df_refined)\n",
        "        current_ranges = analyze_and_refine_ranges(df_refined, shrink_factor=shrink_factor)\n",
        "\n",
        "    # Combine all results\n",
        "    df_all = pd.concat(all_results, ignore_index=True)\n",
        "    df_all.to_csv('all_successful_regions.csv', index=False)\n",
        "    print(\"\\nRecursive parameter search complete! All results saved.\")\n",
        "\n",
        "    return df_all\n",
        "\n",
        "# --- Execute Full Pipeline ---\n",
        "\n",
        "df_results = recursive_parallel_search(\n",
        "    initial_samples=1000,\n",
        "    focused_samples=500,\n",
        "    num_iterations=3,\n",
        "    shrink_factor=1.0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU0nX6Z7yLHL",
        "outputId": "28cf5664-2ed0-4281-8a58-872d21d921e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Recursive Parameter Search (3 Iterations) ---\n",
            "\n",
            "\n",
            "Initial: Running 1000 samples in parallel...\n",
            "Using 8 CPU cores for parallelization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 253/1000 [05:01<15:05,  1.21s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IrQ3NtI4wgb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}